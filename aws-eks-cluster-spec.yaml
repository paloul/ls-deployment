# https://eksctl.io/usage/schema/
#
# Cost-Optimized EKS cluster for Kubeflow with spot GPU instances and node scale down to zero
# Built in efforts to reducing training costs of ML workloads.
# Supporting tutorial can be found at the following link: 
# https://blog.gofynd.com/how-we-reduced-our-ml-training-costs-by-78-a33805cb00cf
# This spec creates a cluster on EKS with the following active nodes 
# - 3x m5a.large   - Accomodates all pods of Kubeflow - no gpu, AMD EPYC7000 2 cpu cores, 8 gb mem
# It also creates the following nodegroups with 0 nodes running unless a pod comes along and requests for the node to get spun up
# - m5a.xlarge    -- Max Allowed 10 worker nodes - no gpu, AMD EPYC7000 4 cpu cores, 16 gb mem
# - m5a.2xlarge   -- Max Allowed 10 worker nodes - no gpu, AMD EPYC7000 8 cpu cores, 32 gb mem
# - p2.xlarge     -- Max Allowed 10 worker nodes - 1 K80 gpu, 4 cpu cores, 61 gb mem, 12 gb gpu mem
# - p3.2xlarge    -- Max Allowed 10 worker nodes - 1 V100 gpu, 8 cpu cores, 61 gb mem, 16 gb gpu mem
# - p3.8xlarge    -- Max Allowed 04 worker nodes - 3 V100 gpu, 32 cpu cores, 244 gb mem, 64 gb gpu mem
# - p3dn.24xlarge -- Max Allowed 01 worker nodes - 8 V100 gpu, 96 cpu cores, 768 gb mem, 256 gb gpu mem

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  # Name of your cluster, change to whatever you find fit.
  # If changed, make sure to change all nodegroup tags from 
  # 'k8s.io/cluster-autoscaler/babylon-2: "owned"' --> 'k8s.io/cluster-autoscaler/your-new-name: "owned"'
  name: hawkeye-1
  # choose your region wisely, this will significantly impact the cost incurred
  region: us-west-2 # Oregon
  # 1.14 Kubernetes version since Kubeflow 1.0 officially supports the same
  version: '1.18' # Downgrading to 1.18 because Kubeflow officially hasnt listed 1.19 support
  tags:
    # Add more cloud tags if needed for billing
    environment: development
    creator: paloul
    project: hawkeye
    cluster: hawkeye-1

iam:
  # https://eksctl.io/usage/schema/#iam-withOIDC
  withOIDC: true

vpc:
  cidr: 10.10.0.0/16
  # https://eksctl.io/usage/vpc-networking/#nat-gateway
  nat:
    gateway: Single # other options: Disable, Single (default)

  # https://eksctl.io/usage/vpc-networking/#managing-access-to-the-kubernetes-api-server-endpoints
  clusterEndpoints:
    publicAccess:  true
    privateAccess: true

# Add all possible AZs to ensure nodes can be spun up in any AZ later on. 
# THIS CAN'T BE CHANGED LATER. YOU WILL HAVE TO CREATE A NEW CLUSTER TO ADD NEW AZ SUPPORT.
# This list applies to the whole clustr and isn't specific to nodegroups
availabilityZones: ["us-west-2a", "us-west-2b"]

# https://eksctl.io/usage/kms-encryption/
#secretsEncryption:
  # KMS key used for envelope encryption of Kubernetes secrets
  #keyARN: arn:aws:kms:us-west-2:<account>:key/<key>

# https://eksctl.io/usage/schema/#fargateProfiles
#fargateProfiles:
  #- name: fp-babylon
    #selectors:
      # All workloads in the "fp-babylon" Kubernetes namespace will be
      # scheduled onto Fargate:
      #- namespace: fp-babylon

nodeGroups:
  - name: ng-1
    desiredCapacity: 3
    minSize: 1
    maxSize: 6 
    volumeSize: 120
    volumeType: gp3
    instanceType: m5a.large
    availabilityZones: ["us-west-2a"]
    labels:
      node-class: "worker-node"
    tags:
      # EC2 tags required for cluster-autoscaler auto-discovery
      k8s.io/cluster-autoscaler/node-template/label/lifecycle: OnDemand
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/hawkeye-1: "owned"
    iam:
      withAddonPolicies:
        efs: true
        fsx: true
        albIngress: true
        autoScaler: true
        cloudWatch: true